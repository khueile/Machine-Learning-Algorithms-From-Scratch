{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "make a neural network that learns to distinguish between an even number and an odd number\n",
    "predict odd.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stochastic gradient descent (SGD)\n",
    "\n",
    "A gradient descent algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a dataset to calculate an estimate of the gradient at each step.\n",
    "\n",
    "\n",
    "#### gradient descent\n",
    "\n",
    "A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data. Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss.\n",
    "\n",
    "#### loss\n",
    "\n",
    "A measure of how far a model's predictions are from its label. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss.\n",
    "\n",
    "#### prediction\n",
    "\n",
    "A model's output when provided with an input example.\n",
    "\n",
    "#### example\n",
    "\n",
    "One row of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+math.exp(-z))\n",
    "\n",
    "def logloss(true_label, predicted_prob):\n",
    "    if true_label == 1:\n",
    "        return -log(predicted_prob)\n",
    "    else:\n",
    "        return -log(1 - predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inputNeuron:\n",
    "    def __init__(self,activation):\n",
    "        '''\n",
    "        inputNeuron contains only an activation value as given, no bias or weights.\n",
    "        \n",
    "        input:\n",
    "        activation: numerical value from 0 to 1\n",
    "        \n",
    "        attri:\n",
    "        activation: numerical value from 0 to 1\n",
    "        '''\n",
    "        self.activation = activation\n",
    "\n",
    "            \n",
    "class neuron:\n",
    "    def __init__(self,prevActs):\n",
    "        '''\n",
    "        neuron contains randomized weights and bias, \n",
    "        and an activation value calculated from said weights and bias\n",
    "        \n",
    "        input:\n",
    "        prevActs: list of activation values from previous layer\n",
    "        \n",
    "        attri:\n",
    "        weights: list of randomized number of weight associated with a node in previous layer\n",
    "        bias: randomized number \n",
    "        activation: calculated from applying the sigmoid function\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.bias = random.random()  #random\n",
    "        sig=(-self.bias)\n",
    "        \n",
    "        self.weights = []\n",
    "        for i in range(len(prevActs)):\n",
    "            \n",
    "            currweight=random.random() #random\n",
    "            \n",
    "            self.weights.append(currweight)\n",
    "            sig+=prevActs[i] * currweight\n",
    "            \n",
    "        self.activation = sigmoid(sig)\n",
    "    \n",
    "    #def changeto(self, weights, bias)\n",
    "       \n",
    "\n",
    "class layer:\n",
    "    def __init__(self, prevActs,numNeuron, inputLayer = False):\n",
    "        '''\n",
    "        a layer is made up of \"numNeuron\" number of neurons.\n",
    "        \n",
    "        input:\n",
    "        numNneuron: number of neurons in a layer\n",
    "        prevActs: list of activations in previous layer\n",
    "        \n",
    "        attri:\n",
    "        neurons: list of all neurons at current layer\n",
    "        acts: list of activations of those neurons at current layer\n",
    "        '''\n",
    "        self.neurons = []\n",
    "        self.acts = []\n",
    "        if inputLayer == False:\n",
    "            for i in range(numNeuron):\n",
    "                currNeuron = neuron(prevActs)\n",
    "                self.neurons.append(currNeuron)\n",
    "                self.acts.append(currNeuron.activation)\n",
    "        else:\n",
    "            for i in range(numNeuron):\n",
    "                currNeuron = inputNeuron(random.random()) #random\n",
    "                self.neurons.append(currNeuron)\n",
    "                self.acts.append(currNeuron.activation)\n",
    "        \n",
    "class network:\n",
    "    def __init__(self, neuronNums):\n",
    "        '''\n",
    "        a network contains an inputLayer, a number of hidden layers and an output layer.\n",
    "        \n",
    "        input:\n",
    "        neuronNums: a list of number of neurons in each consecutive layer of the network\n",
    "        \n",
    "        attributes:\n",
    "        layers: a list of layers in the network.     \n",
    "        '''\n",
    "        self.layers = []\n",
    "        \n",
    "        #input layer\n",
    "        layer1st = layer([],neuronNums[0], inputLayer = True)\n",
    "        self.layers.append(layer1st)\n",
    "        prevActs = layer1st.acts\n",
    "        \n",
    "        #hidden layer(s) + output layer\n",
    "        for numNeuron in neuronNums[1:]:\n",
    "            currLayer = layer(prevActs,numNeuron)\n",
    "            self.layers.append(currLayer)\n",
    "            prevActs = currLayer.acts\n",
    "    \n",
    "    def forward(self, inputActs):\n",
    "        '''\n",
    "        inputActs: list of activation(s) for input layer\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(inputActs)):\n",
    "            self.layers[0].neurons[i].activation = inputActs[i]\n",
    "        prevActs = inputActs\n",
    "        \n",
    "        for nLayer in range(1,len(self.layers)): #for each non-input layer in network\n",
    "            for nNeuron in range(len(self.layers[nLayer].neurons)): #for each neuron in layer\n",
    "                currNeuron = self.layers[nLayer].neurons[nNeuron]\n",
    "                sig=(-currNeuron.bias)\n",
    "                \n",
    "                for w in range(len(prevActs)): #for each weight connection in neuron\n",
    "                    sig += prevActs[w] * currNeuron.weights[w]\n",
    "                    \n",
    "                currNeuron.activation = sigmoid(sig)\n",
    "                currNeuron = self.layers[nLayer].acts[nNeuron] = currNeuron.activation\n",
    "                \n",
    "            prevActs = self.layers[nLayer].acts\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynetwork = network([1,8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7850482634786273"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynetwork.layers[0].neurons[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714564450578488"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynetwork.layers[-1].neurons[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynetwork.forward([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynetwork.layers[0].neurons[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783641662666129"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynetwork.layers[-1].neurons[0].activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make training and testing dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedata(datapointNum):\n",
    "    \"\"\"\n",
    "    Returns \"dataset\": a list of examples.\n",
    "    Each example is a tuple.\n",
    "    Each tuple contains (number,label).\n",
    "    number ranges between 0 and 100001\n",
    "    label 0 is even, 1 is odd.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for i in range(datapointNum):\n",
    "        label=random.randint(0,1)\n",
    "        if label==0:\n",
    "            #generate even number\n",
    "            even = 2*random.randint(0,50000)\n",
    "            dataset.append((even,label))\n",
    "            \n",
    "        else:\n",
    "            #generate odd number\n",
    "            odd = 2*(random.randint(1,50000)//2)+1\n",
    "            dataset.append((odd,label))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57976, 0), (62972, 0), (83850, 0), (17997, 1), (11644, 0)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makedata(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = makedata(3)\n",
    "test = makedata(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:\n",
    "- calculate the cost/loss for each traning datapoint and take their average for cost of the network.\n",
    "- Find gradient descend through back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(network,data):\n",
    "    loss = 0\n",
    "    for dtpt in data:\n",
    "        network.forward([dtpt[0]])\n",
    "        predicted_prob = mynetwork.layers[-1].neurons[0].activation\n",
    "        true_label = dtpt[1]\n",
    "        loss += logloss(true_label, predicted_prob)\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
